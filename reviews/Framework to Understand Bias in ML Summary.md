This paper mainly covers issues of bias that do not directly apply to our model. However, when it does describe the 7 sources of harm in machine learning, it briefly discusses NLP and issues that relate to our project. For historical bias, it touches on how word embeddings can reflect real world biases about women and ethnic minorities, and that when NLP apps are built with these word embeddings, these applications can also reflect stereotypes. The paper also describes aggregation bias, an example of which was a nonspecific model that was analyzing tweets misinterpreting rap lyrics as aggression. Misclassifications like these can arise when general models are used for group-specific contexts.
