<html>
    <p>Summary by: Emilian Sega</p>
    <p>In this paper, the authors show the effects of bias and fairness on machine learning, by demonstrating multiple sources of bias, fairness, and ultimately how to mitigate this. Out of the multitude of biases listed, the one that is most important to our project is social bias, which happens when other people’s actions or content coming from them affect our judgment. In terms of a model and dataset, this occurs when there is already bias in a dataset, in which a model trains on it, and develops those biases. This paper presents a different way to mitigate bias, and this is through penalizing the algorithm by calculating a said fairness value. Depending on how that fairness value looks, different steps can be taken to change the algorithm. I think this is an interesting way of evaluating if a model is biased, but I think it would be hard to implement into our project, because of the fact that it’s hard to implement, and simply due to its sheer complexity. This paper provides a simple and concise summary of another paper read, “Gender Bias in Contextualized Word Embeddings”, which basically has the idea of noticing bias in word embeddings, and using techniques such as data augmentation, as talked in a different paper to mitigate these biases. Since this paper is a review, it didn’t really introduce anything new that could be extremely useful to us, but rather solidified the ideas of the importance of bias, and ways to actually measure and mitigate these. Looking forward, the ideas we should remember from this paper are how to measure fairness and bias, and combining this with some data augmentation techniques.
</p>
</html>
