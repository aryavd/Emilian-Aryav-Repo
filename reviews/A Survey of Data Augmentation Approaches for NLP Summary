<html>
    <p>Summary by: Emilian Sega</p>
    <p>In this paper, the authors explore the method of data augmentation on NLPs, a way of increasing training data diversity without directly collecting more data. This can be implemented using various techniques such as rule based techniques, example interpolation techniques, and model based techniques. The goal of data augmentation is to provide a balance between something that’s easy to implement and use, but also provides an improvement in model performance. Some methods are more useful than others, such as ruled based techniques which focus mainly on text classification, while methods like example interpolation and model based techniques focus on other parts. One method we could use in our project is rule based techniques, or more specifically EDA, Easy Data Augmentation, which are a set of operations that create a new data augmented pair based off of an input. For example, you would input x, and get back a DA(x), being the data augmented pair to the input x. This method is shown to have improved performance on text classification tasks, which is what our project is based on. The other methods talked about in this paper cannot really be used in our project, as they focus on other parts that do not really relate (ie: automated text augmentation, or methods involving images). An idea that we should definitely keep in mind is using data augmentation to create an identical dataset, but with bias towards the underrepresented gender (for example, switching “he” and “she” in a sentence), and then train a model on the union of these two datasets. This is something that could be very beneficial to the project, as using the methods listed above, in combination with the swapped identical dataset, can prove to be an easy task to eliminate bias, improving the model.
</p>
</html>
