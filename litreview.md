•	Fairness in Machine Learning book (https://fairmlbook.org/index.html) 
o	Chapter 2 on classification 
o	Chapter 5 on testing discrimination in practice
•	Gender Bias in Contextualized Word Embeddings (https://arxiv.org/abs/1904.03310)
•	A Survey on Bias and Fairness in Machine Learning (https://arxiv.org/abs/1908.09635)
•	Beyond Accuracy: Behavioral Testing of NLP models with CheckList (https://arxiv.org/abs/2005.04118)
•	What's in the Box? A Preliminary Analysis of Undesirable Content in the Common Crawl Corpus (https://arxiv.org/abs/2105.02732) 

•	A framework for understanding Sources of Harm throughout the Machine Learning Life Cycle (https://arxiv.org/pdf/1901.10002.pdf)
•	In Marianne Bertrand and Sendhil Mullainathan’s audit study, “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination,” fictitious résumés with randomized raced names are sent out to employers looking to hire. Since the résumés are identical except for the differently raced names of the applicants, the difference in callback rates between résumés with names raced white (the Emilys and Gregs) and résumés with names raced black (the Lakishas and Jamals) is interpreted as being due to the decision-makers’ perception of race.
•	The meaning and measurement of bias: lessons from natural language processing (https://dl.acm.org/doi/abs/10.1145/3351095.3375671)
•	A survey of data augmentation approaches for NLP (https://arxiv.org/abs/2105.03075) 
•	Towards Understanding and Mitigating Social Biases in Language Models (https://arxiv.org/abs/2106.13219) 
![image](https://user-images.githubusercontent.com/52499694/128066436-2e71d401-6158-4ba2-b6d5-e80f576656c6.png)
