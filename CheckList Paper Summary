The authors aimed to show how CheckList can be used to guide users on what to test their NLP models on in the form of a list of linguistic capabilities, as well as to reveal previously unknown bugs and failure rates in models completing linguistic tasks. The paper highlights significant problems at multiple levels of the NLP pipeline and how CheckList revealed these failures. It did so by means of matrices: the rows were comprised of different linguistic capabilities, the columns were comprised of different test types, and the cells were filled by generating test cases. CheckList provides users an abstraction where they can mask part of a generalized template and receive masked language model suggestions that they can then filter into positive, negative, and neutral fill in lists and reuse across multiple tests. These suggestions can also be used without filtering or in perturbations, when they can be combined with categories like synonyms and antonyms to make the perturbations more contextually accurate. It also provides additional common fill-ins for general purpose categories. Some benefits of the method include its ability to compare different models trained on different data, or third-party models where access to training data or model structure is not granted as a result of CheckListâ€™s treating of the model as a black box. It also tests on capabilities instead of individual components, which is beneficial because modern NLP models are seldom built one component at a time. Another benefit is that while some of the tests are task-specific, the capabilities and test types are general and can be applied across tasks or with minor variation. Even in situations where models are already tested extensively, CheckList added value and discovered bugs in systems already tested. CheckList users with no previous experience were also able to discover problems, and in addition to being easy to learn and use, the system can be applied to any model and is easy to incorporate in current benchmarks or evaluation pipelines. For our purposes, the two tested capabilities that are most important are Robustness and Fairness. Robustness checked for how the model reacts differently to typos and other unimportant changes in the text, and Fairness checked for how the model reacts differently (positive, negative, or neutral) to protected adjectives that contain info on race, gender, sexuality, religion, etc. We aim to design a similar system that checks for robustness to perturbation of demographic attributes, making both linguistic capabilities earlier described somewhat applicable. Isolating and considering gender-specific words in our dataset in a similar manner to the protected adjectives in the CheckList system will be crucial to developing the test.
